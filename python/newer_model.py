#%% Imports
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
import matplotlib.pyplot as plt

#%% Data loader
batch_size_test = 1000

test_loader = torch.utils.data.DataLoader(
  torchvision.datasets.MNIST('/files/', train=False, download=True,
                             transform=torchvision.transforms.Compose([
                               torchvision.transforms.ToTensor(),
                               torchvision.transforms.Normalize(
                                 (0.1307,), (0.3081,))
                             ])),
  batch_size=batch_size_test, shuffle=True)

examples = enumerate(test_loader)
batch_idx, (example_data, example_targets) = next(examples)

#%% Reference Pytorch model

i = 6
results = []

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)
        self.fc1 = nn.Linear(1690, 10)

    def forward(self, x):
        x = self.conv1(x)
        results.append(x.detach().numpy())

        x = F.max_pool2d(x, kernel_size=2)

        x = F.relu(x)
        results.append(x.detach().numpy())

        x = x.view(-1, 1690)
        results.append(x.detach().numpy())

        x = self.fc1(x)
        results.append(x)

        x = F.log_softmax(x)
        results.append(x)
        
        return x

net = Net()

# Load weights and biases
trained_model = torch.load('new_model.pth')
net.conv1.weight.data = trained_model['conv1.weight']
net.conv1.bias.data = trained_model['conv1.bias']
net.fc1.weight.data = trained_model['fc3.weight']
net.fc1.bias.data = trained_model['fc3.bias']

mnist_example = example_data[i][0].unsqueeze(0).unsqueeze(0)
np.savetxt('latest_mnist.csv', mnist_example.numpy()[0][0])

result = net(mnist_example)
plt.pcolor(np.flip(mnist_example[0][0], axis=0))
plt.title("Pytorch network prediction\n{}".format(np.where(result[0].detach().numpy() == float(min(result[0], key=abs)))))

# Define data input dimensions
CO1_B = 1
CO1_F = 1
CO1_R = 28
CO1_C = 28

# Define kernel dimensions
CO1_K_R = 3
CO1_K_C = 3

# Define convolution output dimensions
CO1_OB = 1
CO1_OF = 10
CO1_OR = 26
CO1_OC = 26

# Define maxpool / ReLU dimensions
MR1_OB = 1
MR1_OF = 10
MR1_OR = 13
MR1_OC = 13

# Set up memory buffers
CO1_OUTPUT = np.zeros([CO1_OB, CO1_OF, CO1_OR, CO1_OC])
MR1_OUTPUT = np.zeros([MR1_OB, MR1_OF, MR1_OR, MR1_OC])
FL1_OUTPUT = np.zeros([1, 1690])
FC1_OUTPUT = np.zeros([1, 10])

# Load model parameters
CO1_KERNEL = trained_model['conv1.weight'].numpy()      # (10, 1, 3, 3)
CO1_BIAS = trained_model['conv1.bias'].numpy()          # (10,)
FC1_WEIGHT = trained_model['fc3.weight'].numpy()        # (10, 1690)
FC1_BIAS = trained_model['fc3.bias'].numpy()            # (10,)

# Correct input data dimensions
data = mnist_example.numpy()

def CO1():
    """Convolution layer"""
    global data, CO1_OUTPUT, CO1_KERNEL

    for b in range(CO1_OB):
        for f in range(CO1_OF):
            # Convolution begins here
            for r in range(CO1_OR):
                for c in range(CO1_OC):
                    # Calculate kernel and receptive field dot product
                    convoluted = 0
                    for k_r in range(CO1_K_R):
                        for k_c in range(CO1_K_C):
                            convoluted += data[b][0][r+k_r][c+k_c] * CO1_KERNEL[f][0][k_r][k_c]

                    CO1_OUTPUT[b][f][r][c] = convoluted + CO1_BIAS[f]

def MR1():
    """Maxpools and ReLUs convolution output product"""
    global CO1_OUTPUT, MR1_OUTPUT

    for b in range(MR1_OB):
        for f in range(MR1_OF):
            # Maxpool begins here
            for r in range(MR1_OR):
                for c in range(MR1_OC):
                    frame = np.zeros(4)
                    frame[0] = CO1_OUTPUT[b][f][r*2  ][c*2  ]
                    frame[1] = CO1_OUTPUT[b][f][r*2+1][c*2  ]
                    frame[2] = CO1_OUTPUT[b][f][r*2  ][c*2+1]
                    frame[3] = CO1_OUTPUT[b][f][r*2+1][c*2+1]

                    # ReLU begins here
                    maximum = 0
                    for i in range(4):
                        if frame[i] > maximum:
                            maximum = frame[i]

                    MR1_OUTPUT[b][f][r][c] = maximum

def FL1():
    """Flattens data to: (1, 1690)"""
    global FL1_OUTPUT

    i = 0
    for b in range(MR1_OB):
        for f in range(MR1_OF):
            for r in range(MR1_OR):
                for c in range(MR1_OC):
                    FL1_OUTPUT[b][i] = MR1_OUTPUT[b][f][r][c]
                    i = i + 1

def FC1():
    """Fully connected layer output dimensions: (1, 10)"""
    global FC1_OUTPUT

    for i in range(10):
        total = np.zeros(1)
        for j in range(1690):
            total += FL1_OUTPUT[0][j] * FC1_WEIGHT[i][j]

        FC1_OUTPUT[0][i] = total + FC1_BIAS[i]


#data = input_data
CO1()
MR1()
FL1()
FC1()

#%% Calculate total pixelwise error
test = CO1_OUTPUT - results[0]
error = 0
for b in range(test.shape[0]):
    for f in range(test.shape[1]):
        for r in range(test.shape[2]):
            for c in range(test.shape[3]):
                error += test[b][f][r][c]
print("Total error in CO1 test: {:>14.10f}".format(error))

test = MR1_OUTPUT - results[1]
error = 0
for b in range(test.shape[0]):
    for f in range(test.shape[1]):
        for r in range(test.shape[2]):
            for c in range(test.shape[3]):
                error += test[b][f][r][c]
print("Total error in MR1 test: {:>14.10f}".format(error))

test = FL1_OUTPUT - results[2]
error = 0
for i in range(test.shape[0]):
    for j in range(test.shape[1]):
        error += test[i][j]
print("Total error in FL1 test: {:>14.10f}".format(error))

test = FC1_OUTPUT - results[3].detach().numpy()
error = 0
for i in range(test.shape[0]):
    for j in range(test.shape[1]):
        error += test[i][j]
print("Total error in FC1 test: {:>14.10f}".format(error))

error = 0
for i in range(10):
    error += F.log_softmax(torch.tensor(FC1_OUTPUT[0][i])).detach().numpy() - F.log_softmax(results[3][0][1]).detach().numpy()
print("Total error in FC1 test: {:>14.10f}".format(error))

#%% Compare results visually
plt.subplot(131)
plt.pcolor(data[0][0])
avg = round(np.mean(data[0][0]), 4)
plt.title("Original\nMean: {:.6f}".format(avg))

plt.subplot(132)
plt.pcolor(results[0][0][0])
avg = round(np.mean(results[0][0][0]), 4)
plt.title("Ref network\nMean: {:.6f}".format(avg))

plt.subplot(133)
plt.pcolor(CO1_OUTPUT[0][0])
avg = round(np.mean(CO1_OUTPUT[0][0]), 4)
plt.title("New network\nMean: {:.6f}".format(avg))

#%% Compare results visually
plt.subplot(121)
plt.pcolor(results[1][0][0])
avg = round(np.mean(results[1][0][0]), 4)
plt.title("Ref network\nMean: {:.6f}".format(avg))

plt.subplot(122)
plt.pcolor(MR1_OUTPUT[0][0])
avg = round(np.mean(MR1_OUTPUT[0][0]), 4)
plt.title("New network\nMean: {:.6f}".format(avg))

#%% End results
r = F.log_softmax(torch.tensor(FC1_OUTPUT[0])).detach().numpy()

plt.subplot(121)
plt.pcolor([results[4][0].detach().numpy()])
avg = round(np.mean(results[4][0].detach().numpy()), 4)
plt.title("Ref network\nMean: {:.6f}".format(avg))

plt.subplot(122)
plt.pcolor([r])
avg = round(np.mean(r), 4)
plt.title("New network\nMean: {:.6f}".format(avg))
#%% Print kernel and bias as c array (copy and paste)
print(str(CO1_KERNEL).replace('[', '{').replace(']', '}'))

#%%
CO1_FPGA=[[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.028102, 1.495684, 4.825801, 6.401407, 3.630309, 0.796113, 0.058445, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.264039, 2.376068, 4.390571, 3.224707, 1.070990, -0.846596, -1.316316, -0.196981, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.250160, 2.935689, 4.352538, 2.072774, -1.271522, -2.525763, -1.669326, -1.019777, -0.317876, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.624884, 3.202677, 3.766115, 1.198931, -1.792197, -2.331817, -4.232712, -3.476098, -0.575119, 0.181963, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.333432, 3.132434, 3.860566, 0.770876, -1.899550, -1.984697, -4.232712, -4.207393, -1.091390, 1.032224, 0.154347, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 1.129140, 1.926192, 0.319170, -2.054905, -2.450792, -4.604044, -4.272148, -1.098435, 1.029433, 0.161043, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.402825, 2.154143, 1.261927, -1.004946, -1.171450, -3.734825, -4.066463, -0.851746, 1.039004, 0.161043, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 2.343907, 2.975298, 0.348403, -2.155010, -4.398624, -4.160948, -1.165289, 1.067744, 0.154347, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 1.651903, 2.922988, 1.079696, -1.685222, -2.462977, -3.225104, -1.092915, 1.295313, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.416704, 2.297533, 1.567742, -1.164652, -2.030938, -3.500171, -2.422783, 0.048122, 0.228000, 1.138394, 2.685871, 2.833065, 1.274691, 0.158305, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.041981, 2.328938, 2.693035, 0.158230, -1.536098, -3.671189, -3.520795, 1.329151, 5.239675, 6.225442, 6.191467, 5.003235, 4.109608, 4.389708, 2.482300, 0.618820, 0.034842, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.818028, 2.267604, 0.935905, -1.815974, -3.268977, -1.864342, 3.585018, 6.454889, 3.579674, 1.091955, -0.564425, -1.491794, -0.870009, 0.345539, -0.342424, -1.359819, -0.116137, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.083617, 1.995263, 1.640878, -0.702532, -1.742651, 0.046511, 2.919609, 3.691156, 0.022543, -1.941281, -2.125190, -2.110626, -1.339427, -0.966285, -1.446719, -1.446736, -0.900914, -0.191564, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.775236, 2.099252, 0.709984, -1.525066, -0.568753, 0.351262, 0.510655, -1.751770, -2.733409, -4.070224, -6.194581, -5.015763, -2.012070, -0.837540, -1.907084, -1.721490, -0.209525, 0.107401, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 1.569637, 1.350887, -0.701874, -1.394402, -0.751662, -0.761723, -1.096990, -2.061210, -4.371080, -4.696251, -0.787760, 2.985211, 2.257853, -1.994794, -4.587962, -2.560158, -0.061910, 0.094085, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.352365, -0.228075, -1.466794, -0.919003, -0.919003, -0.919003, -0.919003, 0.087967, 2.157748, 4.579066, 5.655563, 2.578982, -1.962399, -3.855903, -2.694048, 0.114622, 0.837239, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, -0.154886, -0.498760, -0.817306, -0.919003, -0.919003, -0.919003, -0.919003, -0.437898, 0.806903, 1.238401, -0.285979, -1.979503, -4.173023, -3.627917, -1.004883, 0.319747, 0.094085, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, -0.154886, -0.498760, -0.817306, -0.919003, -0.919003, -0.919003, -0.919003, -0.919003, -0.757881, -0.551693, -1.809659, -3.601172, -3.641979, -1.652120, 0.766802, 0.288262, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, -0.862697, -1.115942, -0.909903, -0.919003, -0.919003, -0.919003, -0.919003, -0.919003, -0.989385, -2.660413, -5.201546, -5.650924, -2.432703, 0.237808, 0.321741, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, -1.814539, -4.039358, -4.226194, -3.062401, -1.477540, -0.969841, -1.640693, -3.227603, -5.803020, -6.371589, -3.902475, -0.869750, 0.852087, 0.850706, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, -0.573248, -2.536619, -3.623709, -4.831675, -6.488194, -7.683381, -7.450187, -6.218535, -3.781869, -1.073962, 1.146890, 0.859069, 0.060606, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, -0.010114, -0.491571, -0.936067, -0.921920, -1.066731, -0.522162, -0.062752, 0.443186, 0.920313, 0.853488, 0.073998, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ]]
#%%
#plt.pcolor(CO1_OUTPUT[0][0]-td)
#plt.pcolor(td)
#%%
MR1_FPGA = [[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 1.495684, 6.401407, 3.630309, 0.058445, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 2.935689, 4.390571, 3.224707, 0.000000, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 3.132434, 3.860566, 1.198931, 0.000000, 1.032224, 0.181963, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.402825, 2.154143, 0.319170, 0.000000, 1.039004, 1.029433, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 2.922988, 2.975298, 0.000000, 1.295313, 1.067744, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 2.328938, 2.693035, 0.000000, 1.329151, 6.225442, 6.191467, 4.389708, 2.482300, 0.034842, 0.000345 ],
[0.000345, 0.000345, 0.083617, 2.267604, 0.935905, 2.919609, 6.454889, 3.579674, 0.000000, 0.345539, 0.000000, 0.000345, 0.000345 ],
[0.000345, 0.000345, 1.569637, 2.099252, 0.000000, 0.510655, 0.000000, 0.000000, 2.985211, 0.000000, 0.000000, 0.107401, 0.000345 ],
[0.000345, 0.000345, 0.352365, 0.000000, 0.000000, 0.000000, 2.157748, 5.655563, 2.578982, 0.000000, 0.837239, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.766802, 0.288262, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000000, 0.000000, 0.000000, 0.000000, 1.146890, 0.859069, 0.850706, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.920313, 0.853488, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ],
[0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345 ]]
#%%
#plt.pcolor(MR1_FPGA)
#plt.pcolor(MR1_OUTPUT[0][0])
MR1_OUTPUT[0][0]- MR1_FPGA
#%%
FL1_FPGA = [0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 1.495684, 6.401407, 3.630309, 0.058445, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 2.935689, 4.390571, 3.224707, 0.000000, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 3.132434, 3.860566, 1.198931, 0.000000, 1.032224, 0.181963, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.402825, 2.154143, 0.319170, 0.000000, 1.039004, 1.029433, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 2.922988, 2.975298, 0.000000, 1.295313, 1.067744, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 2.328938, 2.693035, 0.000000, 1.329151, 6.225442, 6.191467, 4.389708, 2.482300, 0.034842, 0.000345, 0.000345, 0.000345, 0.083617, 2.267604, 0.935905, 2.919609, 6.454889, 3.579674, 0.000000, 0.345539, 0.000000, 0.000345, 0.000345, 0.000345, 0.000345, 1.569637, 2.099252, 0.000000, 0.510655, 0.000000, 0.000000, 2.985211, 0.000000, 0.000000, 0.107401, 0.000345, 0.000345, 0.000345, 0.352365, 0.000000, 0.000000, 0.000000, 2.157748, 5.655563, 2.578982, 0.000000, 0.837239, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.766802, 0.288262, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000000, 0.000000, 0.000000, 0.000000, 1.146890, 0.859069, 0.850706, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.920313, 0.853488, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000345, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.506933, 3.710453, 2.460292, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.202897, 3.315969, 6.121243, 6.042895, 0.285933, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.366891, 3.362293, 2.836105, 0.291133, 0.662825, 0.057726, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.059167, 1.423569, 2.027102, 0.124172, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.722891, 1.676693, 0.044723, 0.191744, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.910285, 1.389926, 0.438772, 0.607764, 3.387847, 4.392393, 5.233191, 2.799801, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.195345, 1.428019, 2.682709, 4.817690, 5.849397, 5.529558, 6.042094, 5.875814, 0.130779, 0.000000, 0.000000, 0.000000, 0.587053, 0.794570, 1.067469, 3.691046, 1.821918, 0.000000, 0.000000, 0.376895, 1.130716, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.633495, 0.633495, 4.793425, 6.440554, 2.596581, 0.014583, 0.035893, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.633495, 0.633495, 1.044196, 1.734811, 0.000000, 0.182421, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.267815, 0.611868, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.894902, 3.013351, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 2.716837, 4.229475, 4.017599, 0.000000, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 3.090037, 4.595055, 3.194935, 0.000000, 0.000000, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.403047, 4.694129, 3.748023, 0.000000, 0.000000, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 4.351650, 4.559447, 0.000000, 0.000000, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 2.624364, 4.660155, 0.606787, 0.000000, 2.083121, 2.209532, 1.406868, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.257223, 4.698862, 4.242844, 0.000000, 1.923462, 2.739620, 1.832726, 0.000000, 0.000000, 1.219182, 1.219182, 1.219182, 1.219182, 2.644502, 4.680782, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.219182, 1.219182, 1.219182, 1.219182, 3.258899, 4.206589, 0.000000, 0.000000, 0.000000, 1.142953, 2.698049, 0.000000, 1.098863, 1.219182, 1.219182, 1.219182, 1.219182, 3.082353, 3.301044, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.806659, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.724919, 1.919093, 0.000000, 0.000000, 0.000000, 0.000000, 1.141834, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 1.219182, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.150824, 3.771365, 2.341685, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.924584, 4.070482, 3.972743, 2.221147, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.468570, 4.252687, 3.968416, 2.809647, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.152641, 3.467909, 3.782165, 2.784676, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 2.206694, 4.014482, 3.178920, 0.197850, 0.133180, 0.726155, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.116677, 4.091598, 4.022326, 2.342696, 3.938488, 4.338418, 4.332737, 2.205851, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 2.614146, 4.234692, 4.243052, 3.965998, 3.759036, 4.075994, 4.163948, 2.081481, 0.000000, 0.000000, 0.000000, 0.000000, 0.366096, 3.576443, 4.315960, 4.315960, 4.082314, 2.780493, 3.402237, 3.261587, 0.359036, 0.000000, 0.000000, 0.000000, 0.000000, 0.366096, 3.590110, 4.315960, 4.315960, 4.315960, 4.052167, 3.805765, 1.467177, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 2.937281, 4.352432, 4.323463, 3.937889, 3.384760, 1.057490, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.566082, 0.764802, 0.235523, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.263328, 2.935711, 0.293450, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.326228, 4.108178, 0.792178, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.000000, 0.000000, 1.111889, 0.128843, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.000000, 0.335553, 0.254385, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.467100, 1.194524, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.019475, 1.673971, 0.761765, 0.649496, 2.197101, 3.667698, 0.179057, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.000000, 0.398957, 0.241494, 0.719332, 0.424358, 0.974425, 3.984076, 0.478476, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.057741, 2.351853, 0.083630, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.000000, 0.000000, 0.222377, 1.352604, 0.000000, 0.891675, 0.252976, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.030325, 1.213161, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.002207, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.011867, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.000000, 0.000000, 0.000000, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.000000, 0.187556, 2.796578, 2.641658, 0.402989, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.000000, 0.047327, 2.600308, 3.044541, 1.456104, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.000000, 2.010356, 2.380706, 1.305173, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.000000, 1.509715, 1.791263, 1.292758, 0.000000, 0.000000, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.000000, 0.000000, 1.017102, 0.000000, 0.000000, 0.649756, 0.345597, 0.000000, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.000000, 0.000000, 0.000000, 2.216683, 3.652582, 4.236810, 2.136773, 1.799756, 0.263731, 0.055758, 0.055758, 0.055758, 0.696537, 1.027361, 0.079848, 0.079848, 0.000000, 0.000000, 0.776785, 2.090102, 2.116557, 0.055758, 0.055758, 0.055758, 0.055758, 1.070468, 1.460318, 0.079848, 0.079848, 0.120590, 2.647892, 4.211719, 1.907564, 0.334736, 0.055758, 0.055758, 0.055758, 0.055758, 1.590011, 4.868404, 6.352413, 6.223027, 5.519948, 4.412807, 3.596971, 0.879717, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.069056, 1.564916, 4.090760, 4.880352, 3.911647, 0.985314, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.055758, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.145912, 0.078624, 0.153479, 0.034541, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.232016, 1.113384, 2.374752, 1.199489, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.230685, 1.875900, 2.566571, 2.298015, 1.465841, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.041734, 1.348885, 2.946676, 2.207432, 2.021961, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.243223, 2.584719, 2.538451, 2.219917, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.229929, 2.526260, 2.729865, 1.955499, 0.000000, 0.383167, 0.421876, 0.176950, 0.020695, 0.000460, 0.000460, 0.000460, 0.008999, 0.855161, 2.891320, 2.431622, 1.537240, 2.183779, 2.377654, 2.219069, 1.109321, 0.000460, 0.000460, 0.000460, 0.000460, 0.195080, 2.845123, 2.735058, 2.241044, 2.381769, 2.325099, 2.362348, 2.437530, 1.116733, 0.000460, 0.000460, 0.000460, 0.000460, 0.747667, 2.715150, 2.244799, 2.244799, 2.230487, 0.933036, 2.459279, 2.357896, 0.000000, 0.000460, 0.000460, 0.000460, 0.000460, 0.747667, 2.583655, 2.244799, 2.244799, 2.244799, 2.270440, 2.315540, 0.258856, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.573195, 2.844612, 2.503744, 2.390724, 2.213326, 2.497030, 0.528748, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.008941, 0.894704, 1.710187, 1.685636, 0.946026, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000460, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.707068, 1.482539, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.518476, 6.084758, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.003503, 2.220701, 4.064229, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.127310, 2.355490, 2.569125, 0.655055, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 2.682972, 3.395114, 0.370764, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 2.001138, 3.741911, 1.314355, 0.789700, 3.001015, 1.798976, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.944224, 4.276767, 3.988518, 0.816904, 0.921322, 3.491092, 6.530212, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.716086, 3.003757, 1.497964, 0.855254, 0.000000, 2.049126, 5.742967, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.244859, 1.201861, 1.201861, 4.076833, 3.593339, 1.265160, 3.336118, 2.120077, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.269893, 1.201861, 1.201861, 1.586276, 1.785885, 3.354107, 2.684280, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.970927, 1.244882, 0.762787, 1.715942, 1.075405, 0.017552, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.956512, 0.101379, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 4.836707, 0.986665, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 3.263803, 5.360054, 0.759847, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 3.498079, 5.347577, 2.067086, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 2.265035, 6.027684, 1.530095, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.123859, 5.476381, 3.518310, 0.000000, 0.000000, 0.279669, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 2.559879, 4.054420, 0.000000, 0.000000, 0.000000, 4.810015, 0.584372, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.286260, 0.530315, 2.146742, 0.127140, 1.726716, 5.560467, 0.452267, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.028335, 2.079529, 0.000000, 4.894602, 4.349429, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.736560, 4.748624, 5.542943, 0.340644, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.067841, 2.424210, 3.706403, 3.339226, 1.035696, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.277089, 2.358661, 1.147312, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.056905, 2.142766, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 4.482717, 4.810778, 1.638189, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 5.455207, 5.176060, 2.391793, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 4.161174, 5.190296, 2.155235, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.571810, 5.460768, 3.186990, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 5.418034, 4.635884, 0.915323, 0.526382, 2.712946, 2.024352, 1.298309, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 3.264020, 5.140254, 2.222590, 4.238056, 3.363396, 3.437119, 1.869798, 0.979069, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 5.199395, 3.480562, 1.744713, 3.233193, 1.636260, 2.976767, 3.243183, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.698952, 3.938991, 1.457087, 1.457087, 0.734636, 3.180059, 4.900544, 1.840343, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.919711, 3.507653, 1.457087, 1.457087, 1.488544, 2.166950, 1.713179, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 1.076319, 2.353049, 0.743151, 1.682175, 1.473823, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000]

#%%
FC1_FPGA = [-2.103843, -4.940839, -0.569165, 2.481354, 2.106628, 1.744732, 18.937033, -12.026017, 1.020164, -7.126944]
#%%
FC1_OUTPUT - FC1_FPGA

#%%
def softmax(x):
    """Compute softmax values for each sets of scores in x."""
    e_x = np.exp(x)# - np.max(x))
    print(e_x.sum())
    print(e_x)
    print(e_x / e_x.sum())
    return np.log(e_x / e_x.sum())

softmax(FC1_FPGA)


